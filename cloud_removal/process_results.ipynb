{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/mnt/c/Users/laure/Data/SEN2-MSI-T/\"\n",
    "result_path = \"/mnt/c/Users/laure/Data/results/performance_measures/\"\n",
    "result_path = \"/mnt/c/Users/Laurens/Downloads/performance_measures/\" # Main\n",
    "#result_path = \"/mnt/c/Users/Laurens/Downloads/performance_measures_l1c/\" # NN comparison experiment\n",
    "\n",
    "\n",
    "scenes = list(os.listdir(result_path))\n",
    "\n",
    "features = [\"1w\", \"1m\", \"3m\", \"6m\"] \n",
    "#features = [\"reconstruction\"] # NN comparison experiment\n",
    "#features = [\" 1m\"]\n",
    "algs = [\"VPint\", \"VPint_no_IP\", \"VPint_no_EB\", \"replacement\", \"regression\", ]\n",
    "        #\"OracleEnsembleVPint\", \"OracleEnsembleNoVPint\", \"PatchOracleEnsembleVPint\", \"PatchOracleEnsembleNoVPint\"]\n",
    "#algs = [\"VPint\"]\n",
    "#algs = [\"VPint\", \"dsen2cr\", \"dsen2cr_nosar\"] # NN comparison experiment\n",
    "\n",
    "measure = \"MAE\"\n",
    "CI_filter = False\n",
    "CI_interval = 95\n",
    "\n",
    "significance_level = 0.05\n",
    "\n",
    "with open(\"invalid_patches.pkl\", 'rb') as fp:\n",
    "        invalid_patches = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0196b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualising specific result patches\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def normalise_and_visualise(img,title=\"\",rgb=[3,2,1],percentile_clip=True,save_fig=False,**kwargs):\n",
    "    \n",
    "    new_img = np.zeros((img.shape[0],img.shape[1],3))\n",
    "    new_img[:,:,0] = img[:,:,rgb[0]]\n",
    "    new_img[:,:,1] = img[:,:,rgb[1]]\n",
    "    new_img[:,:,2] = img[:,:,rgb[2]]\n",
    "    \n",
    "    if(percentile_clip):\n",
    "        min_val = np.nanpercentile(new_img,1)\n",
    "        max_val = np.nanpercentile(new_img,99)\n",
    "\n",
    "        new_img = np.clip((new_img-min_val)/(max_val-min_val),0,1)\n",
    "    \n",
    "    plt.imshow(new_img,interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    if(save_fig):\n",
    "        plt.savefig(kwargs['path'],bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "paths = [\"/mnt/c/Users/laure/Downloads/grace/127.npy\"]\n",
    "\n",
    "for path in paths:\n",
    "    a = np.load(path)\n",
    "    normalise_and_visualise(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b97a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def normalise_and_visualise(img,title=\"\",rgb=[3,2,1],percentile_clip=True,save_fig=False,**kwargs):\n",
    "    \n",
    "    new_img = np.zeros((img.shape[0],img.shape[1],3))\n",
    "    new_img[:,:,0] = img[:,:,rgb[0]]\n",
    "    new_img[:,:,1] = img[:,:,rgb[1]]\n",
    "    new_img[:,:,2] = img[:,:,rgb[2]]\n",
    "    \n",
    "    if(percentile_clip):\n",
    "        min_val = np.nanpercentile(new_img,1)\n",
    "        max_val = np.nanpercentile(new_img,99)\n",
    "\n",
    "        new_img = np.clip((new_img-min_val)/(max_val-min_val),0,1)\n",
    "    \n",
    "    plt.imshow(new_img,interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    if(save_fig):\n",
    "        plt.savefig(kwargs['path'],bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "path = \"/mnt/c/Users/laure/Downloads/grace/227.npy\"\n",
    "img = np.load(path)\n",
    "normalise_and_visualise(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b4bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising example patches per LC\n",
    "\n",
    "base_path = \"/mnt/c/Users/Laurens/Downloads/patch_vis/\"\n",
    "lcs = [\"cropland\", \"forest\", \"herbaceous\", \"shrubs\", \"urban\"]\n",
    "\n",
    "def normalise_and_visualise(img,title=\"\",rgb=[3,2,1],percentile_clip=True,save_fig=False,**kwargs):\n",
    "    \n",
    "    new_img = np.zeros((img.shape[0],img.shape[1],3))\n",
    "    new_img[:,:,0] = img[:,:,rgb[0]]\n",
    "    new_img[:,:,1] = img[:,:,rgb[1]]\n",
    "    new_img[:,:,2] = img[:,:,rgb[2]]\n",
    "    \n",
    "    if(percentile_clip):\n",
    "        min_val = np.nanpercentile(new_img,1)\n",
    "        max_val = np.nanpercentile(new_img,99)\n",
    "\n",
    "        new_img = np.clip((new_img-min_val)/(max_val-min_val),0,1)\n",
    "    \n",
    "    plt.imshow(new_img,interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    if(save_fig):\n",
    "        plt.savefig(kwargs['path'],bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "for lc in lcs:\n",
    "    patch = np.load(base_path + lc + \".npy\")\n",
    "    normalise_and_visualise(patch, save_fig=True, path=\"plots/patch_vis_\"+lc+\".pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5924fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check means per scene per feature\n",
    "\n",
    "base_path = \"/mnt/c/Users/laure/Data/SEN2-MSI-T/\"\n",
    "result_path = \"/mnt/c/Users/laure/Data/results/performance_measures/\"\n",
    "result_path = \"/mnt/c/Users/Laurens/Downloads/performance_measures/\" # Main\n",
    "result_path = \"/mnt/c/Users/Laurens/Downloads/performance_measures_l1c/\" # NN comparison experiment\n",
    "\n",
    "\n",
    "scenes = list(os.listdir(result_path))\n",
    "\n",
    "features = [\" 1w\", \" 1m\", \" 3m\"] # Messed up earlier so features have a leading space\n",
    "features = [\"reconstruction\"] # NN comparison experiment\n",
    "#features = [\" 1m\"]\n",
    "algs = [\"VPint\", \"VPint_no_IP\", \"VPint_no_EB\", \"replacement\", \"regression\", \n",
    "        \"OracleEnsembleVPint\", \"OracleEnsembleNoVPint\", \"PatchOracleEnsembleVPint\", \"PatchOracleEnsembleNoVPint\"]\n",
    "algs = [\"VPint\", \"dsen2cr\", \"dsen2cr_nosar\"] # NN comparison experiment\n",
    "measure = \"MAE\"\n",
    "CI_filter = True\n",
    "CI_interval = 95\n",
    "\n",
    "scenes = list(os.listdir(result_path))\n",
    "\n",
    "show_local = True\n",
    "\n",
    "total_vals = {}\n",
    "for alg in algs:\n",
    "    total_vals[alg] = np.array([])\n",
    "\n",
    "for scene in scenes:\n",
    "    path = result_path + scene + \"/\"\n",
    "    if(show_local):\n",
    "        print(scene)\n",
    "    for feature in features:\n",
    "        if(show_local):\n",
    "            print(feature)\n",
    "        for alg in algs:\n",
    "            if(show_local):\n",
    "                print(alg)\n",
    "            path1 = path + alg + \"_\" + measure + \".csv\"\n",
    "            df = pd.read_csv(path1)\n",
    "            df = df.drop_duplicates(subset=[\"patch\"])\n",
    "            vals = df[feature].values\n",
    "            if(CI_filter):\n",
    "                # Only keep results in specified confidence interval, since input data is not flawless\n",
    "                min_val = np.nanpercentile(vals, (100-CI_interval) / 2)\n",
    "                max_val = np.nanpercentile(vals, 100 - (100-CI_interval) / 2)\n",
    "                vals = vals[vals<max_val]\n",
    "                vals = vals[vals>min_val]\n",
    "\n",
    "            #vals = vals[vals < 5000] # TODO: remove, this is simulating filtering\n",
    "            if(len(vals) > 0):\n",
    "                if(show_local):\n",
    "                    print(np.nanmean(vals), \", \", np.nanstd(vals))\n",
    "                total_vals[alg] = np.concatenate((total_vals[alg], vals))\n",
    "    if(show_local):\n",
    "        print(\"\\n\")\n",
    "\n",
    "print(\"Totals\")\n",
    "for alg in algs:\n",
    "    print(alg)\n",
    "    print(np.nanmean(total_vals[alg]), \", \", np.nanstd(total_vals[alg]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fb549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check means per land cover per feature\n",
    "\n",
    "measure = \"SSIM\"\n",
    "\n",
    "scenes = list(os.listdir(result_path))\n",
    "lcs = [\"cropland\", \"forest\", \"herbaceous\", \"shrubs\", \"urban\"]\n",
    "\n",
    "show_local = False\n",
    "\n",
    "for lc in lcs:\n",
    "    total_vals = {}\n",
    "    for alg in algs:\n",
    "        total_vals[alg] = np.array([])\n",
    "\n",
    "    for scene in scenes:\n",
    "        if(scene.split(\"_\")[1] == lc):\n",
    "            path = result_path + scene + \"/\"\n",
    "            for feature in features:\n",
    "                if(show_local):\n",
    "                    print(feature)\n",
    "                for alg in algs:\n",
    "                    path1 = path + alg + \"_\" + measure + \".csv\"\n",
    "                    try:\n",
    "                        df = pd.read_csv(path1)\n",
    "                        df = df.drop_duplicates(subset=[\"patch\"])\n",
    "                        # Drop patches where coverage was not complete (e.g., due to swath)\n",
    "                        invalid_patches_local = []\n",
    "                        invalid_patches_local.extend(invalid_patches[scene]['target']['coverage'])\n",
    "                        invalid_patches_local.extend(invalid_patches[scene][feature]['coverage'])\n",
    "                        df = df[~df['patch'].isin(invalid_patches_local)]\n",
    "                        # To get rid of i/o problems where writing was off (wrote pm as patch ID). Always affects duplicate rows\n",
    "                        # so will not affect performance to remove these.\n",
    "                        df = df[~df.patch.astype(str).str.isnumeric()] \n",
    "                        vals = df[feature].values.astype(float)\n",
    "                        # Drop inf; we do this for MAPE, where fully black targets (caused by coverage\n",
    "                        # issues) would result in infinite percentage errors\n",
    "                        vals = vals[~np.isinf(vals)]\n",
    "                        vals = vals[~np.isnan(vals)]\n",
    "                        if(CI_filter):\n",
    "                            # Only keep results in specified confidence interval, since input data is not flawless\n",
    "                            min_val = np.nanpercentile(vals, (100-CI_interval) / 2)\n",
    "                            max_val = np.nanpercentile(vals, 100 - (100-CI_interval) / 2)\n",
    "                            vals = vals[vals<max_val]\n",
    "                            vals = vals[vals>min_val]\n",
    "\n",
    "                        if(len(vals) > 0):\n",
    "                            if(show_local):\n",
    "                                print(np.nanmean(vals), \", \", np.nanstd(vals))\n",
    "                            total_vals[alg] = np.concatenate((total_vals[alg], vals))\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print(path)\n",
    "                        dslkfjlskj\n",
    "\n",
    "    print(lc)\n",
    "    for alg in algs:\n",
    "        print(alg)\n",
    "        print(np.nanmean(total_vals[alg]), \", \", np.nanstd(total_vals[alg]))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilcoxon test per land cover per feature\n",
    "\n",
    "\n",
    "lcs = [\"cropland\", \"forest\", \"herbaceous\", \"shrubs\", \"urban\"]\n",
    "\n",
    "measure = \"SSIM\"\n",
    "\n",
    "show_local = False\n",
    "\n",
    "for lc in lcs:\n",
    "    total_vals = {}\n",
    "    for alg in algs:\n",
    "        total_vals[alg] = np.array([])\n",
    "\n",
    "    for scene in scenes:\n",
    "        if(scene.split(\"_\")[1] == lc):\n",
    "            path = result_path + scene + \"/\"\n",
    "            for feature in features:\n",
    "                if(show_local):\n",
    "                    print(feature)\n",
    "                for alg in algs:\n",
    "                    path1 = path + alg + \"_\" + measure + \".csv\"\n",
    "                    try:\n",
    "                        df = pd.read_csv(path1)\n",
    "                        df = df.drop_duplicates(subset=[\"patch\"])\n",
    "                        # Drop patches where coverage was not complete (e.g., due to swath)\n",
    "                        invalid_patches_local = []\n",
    "                        invalid_patches_local.extend(invalid_patches[scene]['target']['coverage'])\n",
    "                        invalid_patches_local.extend(invalid_patches[scene][feature]['coverage'])\n",
    "                        df = df[~df['patch'].isin(invalid_patches_local)]\n",
    "                        # To get rid of i/o problems where writing was off (wrote pm as patch ID). Always affects duplicate rows\n",
    "                        # so will not affect performance to remove these.\n",
    "                        df = df[~df.patch.astype(str).str.isnumeric()] \n",
    "                        vals = df[feature].values.astype(float)\n",
    "                        # Drop inf; we do this for MAPE, where fully black targets (caused by coverage\n",
    "                        # issues) would result in infinite percentage errors\n",
    "                        vals = vals[~np.isinf(vals)]\n",
    "                        vals = vals[~np.isnan(vals)]\n",
    "                        if(CI_filter):\n",
    "                            # Only keep results in specified confidence interval, since input data is not flawless\n",
    "                            min_val = np.nanpercentile(vals, (100-CI_interval) / 2)\n",
    "                            max_val = np.nanpercentile(vals, 100 - (100-CI_interval) / 2)\n",
    "                            vals = vals[vals<max_val]\n",
    "                            vals = vals[vals>min_val]\n",
    "\n",
    "                        if(len(vals) > 0):\n",
    "                            if(show_local):\n",
    "                                print(np.nanmean(vals), \", \", np.nanstd(vals))\n",
    "                            total_vals[alg] = np.concatenate((total_vals[alg], vals))\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print(path)\n",
    "                        dslkfjlskj\n",
    "\n",
    "    print(lc)\n",
    "    # Compare every algorithm to every other algorithm; amount of significantly outperformed methods is rank (higher=better)\n",
    "    num_outperformed = {}\n",
    "    for alg in algs:\n",
    "        num_outperformed[alg] = 0\n",
    "        for alg2 in algs:\n",
    "            vals1 = total_vals[alg]\n",
    "            vals2 = total_vals[alg2]\n",
    "            min_len = min(len(vals1), len(vals2))\n",
    "            vals1 = vals1[:min_len]\n",
    "            vals2 = vals2[:min_len]\n",
    "            if(measure == \"SSIM\" or measure == \"PSNR\"):\n",
    "                # Turn into minimisation\n",
    "                vals1 = -vals1\n",
    "                vals2 = -vals2\n",
    "            if(np.mean(vals1) < np.mean(vals2)):\n",
    "                w_test = wilcoxon(vals1, vals2, alternative=\"less\")\n",
    "                if(w_test[1] < significance_level):\n",
    "                    num_outperformed[alg] = num_outperformed[alg] + 1\n",
    "    print(num_outperformed)\n",
    "\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check means per land cover per feature (ensembling)\n",
    "\n",
    "measure = \"MAE\"\n",
    "\n",
    "scenes = list(os.listdir(result_path))\n",
    "lcs = [\"cropland\", \"forest\", \"herbaceous\", \"shrubs\", \"urban\"]\n",
    "\n",
    "algs = [\"OracleEnsembleVPint\", \"OracleEnsembleNoVPint\", \"PatchOracleEnsembleVPint\", \"PatchOracleEnsembleNoVPint\"]\n",
    "\n",
    "show_local = False\n",
    "\n",
    "for lc in lcs:\n",
    "    total_vals = {}\n",
    "    for alg in algs:\n",
    "        total_vals[alg] = np.array([])\n",
    "\n",
    "    for scene in scenes:\n",
    "        if(scene.split(\"_\")[1] == lc):\n",
    "            path = result_path + scene + \"/\"\n",
    "            for feature in features:\n",
    "                if(show_local):\n",
    "                    print(feature)\n",
    "                for alg in algs:\n",
    "                    path1 = path + alg + \"_\" + measure + \".csv\"\n",
    "                    try:\n",
    "                        df = pd.read_csv(path1)\n",
    "                        df = df.drop_duplicates(subset=[\"patch\"])\n",
    "                        # Drop patches where coverage was not complete (e.g., due to swath)\n",
    "                        invalid_patches_local = []\n",
    "                        invalid_patches_local.extend(invalid_patches[scene]['target']['coverage'])\n",
    "                        invalid_patches_local.extend(invalid_patches[scene][feature]['coverage'])\n",
    "                        df = df[~df['patch'].isin(invalid_patches_local)]\n",
    "                        # To get rid of i/o problems where writing was off (wrote pm as patch ID). Always affects duplicate rows\n",
    "                        # so will not affect performance to remove these.\n",
    "                        df = df[~df.patch.astype(str).str.isnumeric()] \n",
    "                        vals = df[feature].values.astype(float)\n",
    "                        # Drop inf; we do this for MAPE, where fully black targets (caused by coverage\n",
    "                        # issues) would result in infinite percentage errors\n",
    "                        vals = vals[~np.isinf(vals)]\n",
    "                        vals = vals[~np.isnan(vals)]\n",
    "                        if(CI_filter):\n",
    "                            # Only keep results in specified confidence interval, since input data is not flawless\n",
    "                            min_val = np.nanpercentile(vals, (100-CI_interval) / 2)\n",
    "                            max_val = np.nanpercentile(vals, 100 - (100-CI_interval) / 2)\n",
    "                            vals = vals[vals<max_val]\n",
    "                            vals = vals[vals>min_val]\n",
    "\n",
    "                        if(len(vals) > 0):\n",
    "                            if(show_local):\n",
    "                                print(np.nanmean(vals), \", \", np.nanstd(vals))\n",
    "                            total_vals[alg] = np.concatenate((total_vals[alg], vals))\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print(path)\n",
    "                        dslkfjlskj\n",
    "\n",
    "    print(lc)\n",
    "    for alg in algs:\n",
    "        print(alg)\n",
    "        print(np.nanmean(total_vals[alg]), \", \", np.nanstd(total_vals[alg]))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilcoxon test per land cover per feature (ensembling)\n",
    "\n",
    "\n",
    "lcs = [\"cropland\", \"forest\", \"herbaceous\", \"shrubs\", \"urban\"]\n",
    "\n",
    "measure = \"MAE\"\n",
    "\n",
    "show_local = False\n",
    "\n",
    "for lc in lcs:\n",
    "    total_vals = {}\n",
    "    for alg in algs:\n",
    "        total_vals[alg] = np.array([])\n",
    "\n",
    "    for scene in scenes:\n",
    "        if(scene.split(\"_\")[1] == lc):\n",
    "            path = result_path + scene + \"/\"\n",
    "            for feature in features:\n",
    "                if(show_local):\n",
    "                    print(feature)\n",
    "                for alg in algs:\n",
    "                    path1 = path + alg + \"_\" + measure + \".csv\"\n",
    "                    try:\n",
    "                        df = pd.read_csv(path1)\n",
    "                        df = df.drop_duplicates(subset=[\"patch\"])\n",
    "                        # Drop patches where coverage was not complete (e.g., due to swath)\n",
    "                        invalid_patches_local = []\n",
    "                        invalid_patches_local.extend(invalid_patches[scene]['target']['coverage'])\n",
    "                        invalid_patches_local.extend(invalid_patches[scene][feature]['coverage'])\n",
    "                        df = df[~df['patch'].isin(invalid_patches_local)]\n",
    "                        # To get rid of i/o problems where writing was off (wrote pm as patch ID). Always affects duplicate rows\n",
    "                        # so will not affect performance to remove these.\n",
    "                        df = df[~df.patch.astype(str).str.isnumeric()] \n",
    "                        vals = df[feature].values.astype(float)\n",
    "                        # Drop inf; we do this for MAPE, where fully black targets (caused by coverage\n",
    "                        # issues) would result in infinite percentage errors\n",
    "                        vals = vals[~np.isinf(vals)]\n",
    "                        vals = vals[~np.isnan(vals)]\n",
    "                        if(CI_filter):\n",
    "                            # Only keep results in specified confidence interval, since input data is not flawless\n",
    "                            min_val = np.nanpercentile(vals, (100-CI_interval) / 2)\n",
    "                            max_val = np.nanpercentile(vals, 100 - (100-CI_interval) / 2)\n",
    "                            vals = vals[vals<max_val]\n",
    "                            vals = vals[vals>min_val]\n",
    "\n",
    "                        if(len(vals) > 0):\n",
    "                            if(show_local):\n",
    "                                print(np.nanmean(vals), \", \", np.nanstd(vals))\n",
    "                            total_vals[alg] = np.concatenate((total_vals[alg], vals))\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print(path)\n",
    "                        dslkfjlskj\n",
    "\n",
    "    print(lc)\n",
    "    # Compare every algorithm to every other algorithm; amount of significantly outperformed methods is rank (higher=better)\n",
    "    num_outperformed = {}\n",
    "    for alg in algs:\n",
    "        num_outperformed[alg] = 0\n",
    "        for alg2 in algs:\n",
    "            vals1 = total_vals[alg]\n",
    "            vals2 = total_vals[alg2]\n",
    "            min_len = min(len(vals1), len(vals2))\n",
    "            vals1 = vals1[:min_len]\n",
    "            vals2 = vals2[:min_len]\n",
    "            if(measure == \"SSIM\" or measure == \"PSNR\"):\n",
    "                # Turn into minimisation\n",
    "                vals1 = -vals1\n",
    "                vals2 = -vals2\n",
    "            if(np.mean(vals1) < np.mean(vals2)):\n",
    "                w_test = wilcoxon(vals1, vals2, alternative=\"less\")\n",
    "                if(w_test[1] < significance_level):\n",
    "                    num_outperformed[alg] = num_outperformed[alg] + 1\n",
    "    print(num_outperformed)\n",
    "\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check means per land cover per feature (L1C)\n",
    "\n",
    "measure = \"SSIM\"\n",
    "\n",
    "lcs = [\"cropland\", \"forest\", \"herbaceous\", \"shrubs\", \"urban\"]\n",
    "\n",
    "result_path_l1c = \"/mnt/c/Users/Laurens/Downloads/performance_measures_l1c/\"\n",
    "scenes_l1c = list(os.listdir(result_path_l1c))\n",
    "\n",
    "features = [\"reconstruction\"] \n",
    "algs = [\"VPint\", \"dsen2cr\", \"dsen2cr_nosar\"] \n",
    "\n",
    "\n",
    "show_local = False\n",
    "\n",
    "for lc in lcs:\n",
    "    total_vals = {}\n",
    "    for alg in algs:\n",
    "        total_vals[alg] = np.array([])\n",
    "\n",
    "    for scene in scenes_l1c:\n",
    "        if(scene.split(\"_\")[1] == lc):\n",
    "            path = result_path_l1c + scene + \"/\"\n",
    "            for feature in features:\n",
    "                if(show_local):\n",
    "                    print(feature)\n",
    "                for alg in algs:\n",
    "                    path1 = path + alg + \"_\" + measure + \".csv\"\n",
    "                    try:\n",
    "                        df = pd.read_csv(path1)\n",
    "                        df = df.drop_duplicates(subset=[\"patch\"])\n",
    "                        # Drop patches where coverage was not complete (e.g., due to swath)\n",
    "                        invalid_patches_local = []\n",
    "                        #invalid_patches_local.extend(invalid_patches[scene]['target']['coverage'])\n",
    "                        #invalid_patches_local.extend(invalid_patches[scene]['1m']['coverage'])\n",
    "                        df = df[~df['patch'].isin(invalid_patches_local)]\n",
    "                        # To get rid of i/o problems where writing was off (wrote pm as patch ID). Always affects duplicate rows\n",
    "                        # so will not affect performance to remove these.\n",
    "                        df = df[~df.patch.astype(str).str.isnumeric()] \n",
    "                        vals = df[feature].values.astype(float)\n",
    "                        # Drop inf; we do this for MAPE, where fully black targets (caused by coverage\n",
    "                        # issues) would result in infinite percentage errors\n",
    "                        vals = vals[~np.isinf(vals)]\n",
    "                        vals = vals[~np.isnan(vals)]\n",
    "                        if(CI_filter):\n",
    "                            # Only keep results in specified confidence interval, since input data is not flawless\n",
    "                            min_val = np.nanpercentile(vals, (100-CI_interval) / 2)\n",
    "                            max_val = np.nanpercentile(vals, 100 - (100-CI_interval) / 2)\n",
    "                            vals = vals[vals<max_val]\n",
    "                            vals = vals[vals>min_val]\n",
    "\n",
    "                        if(len(vals) > 0):\n",
    "                            if(show_local):\n",
    "                                print(np.nanmean(vals), \", \", np.nanstd(vals))\n",
    "                            total_vals[alg] = np.concatenate((total_vals[alg], vals))\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print(path)\n",
    "                        dslkfjlskj\n",
    "\n",
    "    print(lc)\n",
    "    for alg in algs:\n",
    "        print(alg)\n",
    "        print(np.nanmean(total_vals[alg]), \", \", np.nanstd(total_vals[alg]))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilcoxon test per land cover per feature (L1C)\n",
    "\n",
    "\n",
    "lcs = [\"cropland\", \"forest\", \"herbaceous\", \"shrubs\", \"urban\"]\n",
    "\n",
    "result_path_l1c = \"/mnt/c/Users/Laurens/Downloads/performance_measures_l1c/\"\n",
    "scenes_l1c = list(os.listdir(result_path_l1c))\n",
    "\n",
    "features = [\"reconstruction\"] \n",
    "algs = [\"VPint\", \"dsen2cr\", \"dsen2cr_nosar\"] \n",
    "\n",
    "show_local = False\n",
    "\n",
    "for lc in lcs:\n",
    "    total_vals = {}\n",
    "    for alg in algs:\n",
    "        total_vals[alg] = np.array([])\n",
    "\n",
    "    for scene in scenes_l1c:\n",
    "        if(scene.split(\"_\")[1] == lc):\n",
    "            path = result_path_l1c + scene + \"/\"\n",
    "            for feature in features:\n",
    "                if(show_local):\n",
    "                    print(feature)\n",
    "                for alg in algs:\n",
    "                    path1 = path + alg + \"_\" + measure + \".csv\"\n",
    "                    try:\n",
    "                        df = pd.read_csv(path1)\n",
    "                        df = df.drop_duplicates(subset=[\"patch\"])\n",
    "                        # Drop patches where coverage was not complete (e.g., due to swath)\n",
    "                        invalid_patches_local = []\n",
    "                        #invalid_patches_local.extend(invalid_patches[scene]['target']['coverage'])\n",
    "                        #invalid_patches_local.extend(invalid_patches[scene]['1m']['coverage'])\n",
    "                        df = df[~df['patch'].isin(invalid_patches_local)]\n",
    "                        # To get rid of i/o problems where writing was off (wrote pm as patch ID). Always affects duplicate rows\n",
    "                        # so will not affect performance to remove these.\n",
    "                        df = df[~df.patch.astype(str).str.isnumeric()] \n",
    "                        vals = df[feature].values.astype(float)\n",
    "                        # Drop inf; we do this for MAPE, where fully black targets (caused by coverage\n",
    "                        # issues) would result in infinite percentage errors\n",
    "                        vals = vals[~np.isinf(vals)]\n",
    "                        vals = vals[~np.isnan(vals)]\n",
    "                        if(CI_filter):\n",
    "                            # Only keep results in specified confidence interval, since input data is not flawless\n",
    "                            min_val = np.nanpercentile(vals, (100-CI_interval) / 2)\n",
    "                            max_val = np.nanpercentile(vals, 100 - (100-CI_interval) / 2)\n",
    "                            vals = vals[vals<max_val]\n",
    "                            vals = vals[vals>min_val]\n",
    "\n",
    "                        if(len(vals) > 0):\n",
    "                            if(show_local):\n",
    "                                print(np.nanmean(vals), \", \", np.nanstd(vals))\n",
    "                            total_vals[alg] = np.concatenate((total_vals[alg], vals))\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print(path)\n",
    "                        dslkfjlskj\n",
    "\n",
    "    print(lc)\n",
    "    # Compare every algorithm to every other algorithm; amount of significantly outperformed methods is rank (higher=better)\n",
    "    num_outperformed = {}\n",
    "    for alg in algs:\n",
    "        num_outperformed[alg] = 0\n",
    "        for alg2 in algs:\n",
    "            vals1 = total_vals[alg]\n",
    "            vals2 = total_vals[alg2]\n",
    "            min_len = min(len(vals1), len(vals2))\n",
    "            vals1 = vals1[:min_len]\n",
    "            vals2 = vals2[:min_len]\n",
    "            if(measure == \"SSIM\" or measure == \"PSNR\"):\n",
    "                # Turn into minimisation\n",
    "                vals1 = -vals1\n",
    "                vals2 = -vals2\n",
    "            if(np.mean(vals1) < np.mean(vals2)):\n",
    "                w_test = wilcoxon(vals1, vals2, alternative=\"less\")\n",
    "                if(w_test[1] < significance_level):\n",
    "                    num_outperformed[alg] = num_outperformed[alg] + 1\n",
    "    print(num_outperformed)\n",
    "\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e3052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplot per land cover class\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "pretty_names_lc = {\n",
    "    \"urban\":\"Urban\",\n",
    "    \"cropland\":\"Cropland\",\n",
    "    \"herbaceous\":\"Herbaceous\",\n",
    "    \"shrubs\":\"Shrubs\",\n",
    "    \"forest\":\"Forest\",\n",
    "}\n",
    "pretty_names_methods = {\n",
    "    \"VPint\":\"VPint2\",\n",
    "    \"regression\":\"AutoML regression\",\n",
    "    \"regression_band\":\"AutoML regression\",\n",
    "    \"replacement\":\"Replacement\"\n",
    "}\n",
    "\n",
    "algs = [\"VPint\", \"VPint_no_IP\", \"VPint_no_EB\", \"replacement\", \"regression\"]\n",
    "algs = [\"VPint\", \"regression\", \"replacement\"]\n",
    "lcs = [\"cropland\", \"urban\", \"forest\", \"shrubs\", \"herbaceous\"]\n",
    "measure = \"MAE\"\n",
    "\n",
    "\n",
    "scenes = os.listdir(result_path)\n",
    "\n",
    "show_local = False\n",
    "\n",
    "total_vals = {}\n",
    "for alg in algs:\n",
    "    total_vals[alg] = np.array([])\n",
    "\n",
    "total_scenes = {}\n",
    "for alg in algs:\n",
    "    total_scenes[alg] = []\n",
    "\n",
    "for scene in scenes:\n",
    "    path = result_path + scene + \"/\"\n",
    "    for feature in features:\n",
    "        valset = []\n",
    "        for alg in algs:\n",
    "            path1 = path + alg + \"_\" + measure + \".csv\"\n",
    "            try:\n",
    "                df = pd.read_csv(path1)\n",
    "            except Exception as e:\n",
    "                print(e) \n",
    "                print(path1)\n",
    "                sldkfj\n",
    "            df = df.drop_duplicates(subset=[\"patch\"])\n",
    "            # Drop patches where coverage was not complete (e.g., due to swath)\n",
    "            invalid_patches_local = []\n",
    "            invalid_patches_local.extend(invalid_patches[scene]['target']['coverage'])\n",
    "            invalid_patches_local.extend(invalid_patches[scene][feature]['coverage'])\n",
    "            df = df[~df['patch'].isin(invalid_patches_local)]\n",
    "            # To get rid of i/o problems where writing was off (wrote pm as patch ID). Always affects duplicate rows\n",
    "            # so will not affect performance to remove these.\n",
    "            df = df[~df.patch.astype(str).str.isnumeric()] \n",
    "            vals = df[feature].values\n",
    "            if(CI_filter):\n",
    "                # Only keep results in specified confidence interval, since input data is not flawless\n",
    "                min_val = np.nanpercentile(vals, (100-CI_interval) / 2)\n",
    "                max_val = np.nanpercentile(vals, 100 - (100-CI_interval) / 2)\n",
    "                vals = vals[vals<max_val]\n",
    "                vals = vals[vals>min_val]\n",
    "\n",
    "            #vals = vals[vals < 5000] # TODO: remove, this is simulating filtering\n",
    "            if(len(vals) > 0):\n",
    "                total_vals[alg] = np.concatenate((total_vals[alg], vals))\n",
    "                total_scenes[alg].extend([scene for _ in range(0, len(vals))])\n",
    "\n",
    "\n",
    "total_vals_all = np.array([])\n",
    "total_scenes_all = []\n",
    "total_lcs_all = []\n",
    "total_algs_all = []\n",
    "for alg in algs:\n",
    "    total_vals_all = np.concatenate((total_vals_all, total_vals[alg]))\n",
    "    total_scenes_all.extend([a.split(\"_\")[2] for a in total_scenes[alg]])\n",
    "    total_lcs_all.extend([a.split(\"_\")[1] for a in total_scenes[alg]])\n",
    "    total_algs_all.extend([alg for _ in range(0, len(total_vals[alg]))])\n",
    "\n",
    "total_algs_all = [pretty_names_methods[a] for a in total_algs_all]\n",
    "df = pd.DataFrame({\"Algorithm\":total_algs_all, \"Location\":total_scenes_all, \"Land cover\":total_lcs_all,\n",
    "                    measure:total_vals_all})\n",
    "\n",
    "df['Land cover'] = df['Land cover'].apply(lambda x: pretty_names_lc[x])\n",
    "\n",
    "\n",
    "#for lc in lcs:\n",
    "#    df_local = df[df['Land cover'] == lc]\n",
    "#plt.figure(figsize=(20,8))\n",
    "p = sns.catplot(x='Land cover', y=measure, hue='Algorithm', kind='box', data=df)\n",
    "p.fig.suptitle(measure + \" per land cover class\", y=1.02)\n",
    "plt.ylim((0, 3500))\n",
    "plt.savefig(\"plots/box_plot.pdf\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d157a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CD diagram (using autorank)\n",
    "\n",
    "import autorank as ar\n",
    "from autorank import autorank, plot_stats\n",
    "\n",
    "scenes = list(os.listdir(result_path))\n",
    "\n",
    "pretty_names_methods = {\n",
    "    \"VPint\":\"VPint2\",\n",
    "    \"VPint_no_IP\":\"VPint2 (no IP)\",\n",
    "    \"VPint_no_EB\":\"VPint2 (no EB)\",\n",
    "    \"regression\":\"AutoML regression\",\n",
    "    \"regression_band\":\"AutoML regression\",\n",
    "    \"replacement\":\"Replacement\",\n",
    "}\n",
    "\n",
    "#features = [\" 1m\"]\n",
    "algs = [\"VPint\", \"VPint_no_IP\", \"VPint_no_EB\", \"replacement\", \"regression_band\"]\n",
    "measures = [\"MAE\"]\n",
    "significance_level = 0.05\n",
    "\n",
    "\n",
    "for measure in measures:\n",
    "\n",
    "    total_vals = {}\n",
    "    for alg in algs:\n",
    "        total_vals[pretty_names_methods[alg]] = np.array([])\n",
    "\n",
    "    for scene in scenes:\n",
    "        path = result_path + scene + \"/\"\n",
    "        for feature in features:\n",
    "            # First iterate over results per algorithm, store mean PM to be able to compute ranking\n",
    "            alg_results = {}\n",
    "            for alg in algs:\n",
    "                path1 = path + alg + \"_\" + measure + \".csv\"\n",
    "                df = pd.read_csv(path1)\n",
    "                df = df.drop_duplicates(subset=[\"patch\"])\n",
    "                # Drop patches where coverage was not complete (e.g., due to swath)\n",
    "                invalid_patches_local = []\n",
    "                invalid_patches_local.extend(invalid_patches[scene]['target']['coverage'])\n",
    "                if(feature != \"6m\"):\n",
    "                    invalid_patches_local.extend(invalid_patches[scene][feature]['coverage'])\n",
    "                df = df[~df['patch'].isin(invalid_patches_local)]\n",
    "                # To get rid of i/o problems where writing was off (wrote pm as patch ID). Always affects duplicate rows\n",
    "                # so will not affect performance to remove these.\n",
    "                df = df[~df.patch.astype(str).str.isnumeric()] \n",
    "                vals = df[feature].values.astype(float)\n",
    "                vals = vals[~np.isnan(vals)]\n",
    "                if(CI_filter):\n",
    "                    # Only keep results in specified confidence interval, since input data is not flawless\n",
    "                    min_val = np.nanpercentile(vals, (100-CI_interval) / 2)\n",
    "                    max_val = np.nanpercentile(vals, 100 - (100-CI_interval) / 2)\n",
    "                    vals = vals[vals<max_val]\n",
    "                    vals = vals[vals>min_val]\n",
    "\n",
    "                if(len(vals) > 0):\n",
    "\n",
    "                    if(measure == \"SSIM\" or measure == \"PSNR\"):\n",
    "                        # To easily compute ranking for the same objective\n",
    "                        # Autorank automatically does maximisation, so flipping the negative in this cell\n",
    "                        mean_val = np.mean(vals)\n",
    "                    else:\n",
    "                        mean_val = -np.mean(vals)\n",
    "\n",
    "                    total_vals[pretty_names_methods[alg]] = np.concatenate((total_vals[pretty_names_methods[alg]], np.array([mean_val])))\n",
    "    \n",
    "                    \n",
    "    # Draw CD diagram\n",
    "    df = pd.DataFrame(total_vals)\n",
    "    result = autorank(df, alpha=significance_level, verbose=False)\n",
    "    #print(result)\n",
    "\n",
    "    #plot_stats(result)\n",
    "    ar._util.cd_diagram(result, reverse=False, ax=None, width=6) # Same as result but allowing reverse (1 on left)\n",
    "    plt.savefig(\"plots/CD_\" + measure + \".pdf\", bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CD diagram (every patch a condition)\n",
    "\n",
    "import autorank as ar\n",
    "from autorank import autorank, plot_stats\n",
    "\n",
    "scenes = list(os.listdir(result_path))\n",
    "\n",
    "pretty_names_methods = {\n",
    "    \"VPint\":\"VPint2\",\n",
    "    \"VPint_no_IP\":\"VPint2 (no IP)\",\n",
    "    \"VPint_no_EB\":\"VPint2 (no EB)\",\n",
    "    \"regression\":\"AutoML regression\",\n",
    "    \"regression_band\":\"AutoML regression\",\n",
    "    \"replacement\":\"Replacement\",\n",
    "}\n",
    "\n",
    "#features = [\" 1m\"]\n",
    "algs = [\"VPint\", \"VPint_no_IP\", \"VPint_no_EB\", \"replacement\", \"regression_band\"]\n",
    "measures = [\"MAE\"]\n",
    "significance_level = 0.05\n",
    "\n",
    "\n",
    "for measure in measures:\n",
    "\n",
    "    total_vals = {}\n",
    "    for alg in algs:\n",
    "        total_vals[pretty_names_methods[alg]] = np.array([])\n",
    "\n",
    "    for scene in scenes:\n",
    "        path = result_path + scene + \"/\"\n",
    "        for feature in features:\n",
    "            # First iterate over results per algorithm, store mean PM to be able to compute ranking\n",
    "            alg_results = {}\n",
    "            for alg in algs:\n",
    "                path1 = path + alg + \"_\" + measure + \".csv\"\n",
    "                df = pd.read_csv(path1)\n",
    "                df = df.drop_duplicates(subset=[\"patch\"])\n",
    "                # Drop patches where coverage was not complete (e.g., due to swath)\n",
    "                invalid_patches_local = []\n",
    "                invalid_patches_local.extend(invalid_patches[scene]['target']['coverage'])\n",
    "                invalid_patches_local.extend(invalid_patches[scene][feature]['coverage'])\n",
    "                df = df[~df['patch'].isin(invalid_patches_local)]\n",
    "                # To get rid of i/o problems where writing was off (wrote pm as patch ID). Always affects duplicate rows\n",
    "                # so will not affect performance to remove these.\n",
    "                df = df[~df.patch.astype(str).str.isnumeric()] \n",
    "                vals = df[feature].values.astype(float)\n",
    "                vals = vals[~np.isnan(vals)]\n",
    "                if(CI_filter):\n",
    "                    # Only keep results in specified confidence interval, since input data is not flawless\n",
    "                    min_val = np.nanpercentile(vals, (100-CI_interval) / 2)\n",
    "                    max_val = np.nanpercentile(vals, 100 - (100-CI_interval) / 2)\n",
    "                    vals = vals[vals<max_val]\n",
    "                    vals = vals[vals>min_val]\n",
    "\n",
    "                if(len(vals) > 0):\n",
    "\n",
    "                    if(measure == \"SSIM\" or measure == \"PSNR\"):\n",
    "                        # To easily compute ranking for the same objective\n",
    "                        # Autorank automatically does maximisation, so flipping the negative in this cell\n",
    "                        pass\n",
    "                    else:\n",
    "                        vals = vals * -1\n",
    "\n",
    "                    total_vals[pretty_names_methods[alg]] = np.concatenate((total_vals[pretty_names_methods[alg]], vals))\n",
    "\n",
    "    for k,v in total_vals.items():\n",
    "        if(len(v) != 13151):\n",
    "            total_vals[k] = v[:13151] # TODO: remove\n",
    "    \n",
    "                    \n",
    "    # Draw CD diagram\n",
    "    df = pd.DataFrame(total_vals)\n",
    "    result = autorank(df, alpha=significance_level, verbose=False)\n",
    "    #print(result)\n",
    "\n",
    "    #plot_stats(result)\n",
    "    ar._util.cd_diagram(result, reverse=False, ax=None, width=6) # Same as result but allowing reverse (1 on left)\n",
    "    plt.savefig(\"plots/CD_\" + measure + \"_patch.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9556d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise correlation between temporal distance and performance\n",
    "\n",
    "\n",
    "pretty_names_methods = {\n",
    "    \"VPint\":\"VPint2\",\n",
    "    \"VPint_no_IP\":\"VPint2 (no IP)\",\n",
    "    \"VPint_no_EB\":\"VPint2 (no EB)\",\n",
    "    \"regression\":\"AutoML regression\",\n",
    "    \"regression_band\":\"AutoML regression\",\n",
    "    \"replacement\":\"Replacement\"\n",
    "}\n",
    "\n",
    "plot_kwargs_methods = {\n",
    "    \"VPint\":{'color':'#377eb8', 'marker':'o'},\n",
    "    \"VPint_no_IP\":{'color':'#e41a1c', 'marker':'v'},\n",
    "    \"VPint_no_EB\":{'color':'#4daf4a', 'marker':'^'},\n",
    "    \"regression\":{'color':'#984ea3', 'marker':'s'},\n",
    "    \"regression_band\":{'color':'#984ea3', 'marker':'s'},\n",
    "    \"replacement\":{'color':'#ff7f00', 'marker':'P'},\n",
    "}\n",
    "\n",
    "#features = [\" 1m\"]\n",
    "algs = [\"VPint\", \"VPint_no_IP\", \"VPint_no_EB\", \"replacement\", \"regression\"]\n",
    "algs = [\"VPint\", \"regression\", \"replacement\"]\n",
    "measure = \"MAE\"\n",
    "\n",
    "show_local = False\n",
    "\n",
    "\n",
    "for alg in algs:\n",
    "    feature_vals = {\"1w\":np.array([]), \"1m\":np.array([]), \"3m\":np.array([]), \"6m\":np.array([])}\n",
    "    total_months = [0.23, 1, 3, 6]\n",
    "    for scene in scenes:\n",
    "        path = result_path + scene + \"/\"\n",
    "        for feature in features:\n",
    "            valset = []\n",
    "            path1 = path + alg + \"_\" + measure + \".csv\"\n",
    "            df = pd.read_csv(path1)\n",
    "            df = df.drop_duplicates(subset=[\"patch\"])\n",
    "            # Drop patches where coverage was not complete (e.g., due to swath)\n",
    "            invalid_patches_local = []\n",
    "            invalid_patches_local.extend(invalid_patches[scene]['target']['coverage'])\n",
    "            invalid_patches_local.extend(invalid_patches[scene][feature]['coverage'])\n",
    "            df = df[~df['patch'].isin(invalid_patches_local)]\n",
    "            # To get rid of i/o problems where writing was off (wrote pm as patch ID). Always affects duplicate rows\n",
    "            # so will not affect performance to remove these.\n",
    "            df = df[~df.patch.astype(str).str.isnumeric()] \n",
    "            vals = df[feature].values\n",
    "            if(CI_filter):\n",
    "                # Only keep results in specified confidence interval, since input data is not flawless\n",
    "                min_val = np.nanpercentile(vals, (100-CI_interval) / 2)\n",
    "                max_val = np.nanpercentile(vals, 100 - (100-CI_interval) / 2)\n",
    "                vals = vals[vals<max_val]\n",
    "                vals = vals[vals>min_val]\n",
    "\n",
    "            #vals = vals[vals < 5000] # TODO: remove, this is simulating filtering\n",
    "            if(len(vals) > 0):\n",
    "                feature_vals[feature] = np.concatenate((feature_vals[feature], vals))\n",
    "\n",
    "    feature_means = [np.nanmean(v) for k,v in feature_vals.items()]\n",
    "    print(feature_means)\n",
    "    plt.plot(total_months, feature_means, label=pretty_names_methods[alg], **plot_kwargs_methods[alg])\n",
    "\n",
    "plt.xlabel(\"Temporal distance in months\")\n",
    "plt.ylabel(\"Median absolute error\")\n",
    "plt.title(\"Performance per temporal distance\")\n",
    "plt.xlim((0, 6.5))\n",
    "plt.ylim((0, 1200))\n",
    "plt.legend()\n",
    "plt.savefig(\"plots/var_temporal_distance.pdf\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7157653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise correlation between proportion of clouds and performance\n",
    "\n",
    "import random\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "with open(\"cloud_proportions.pkl\", 'rb') as fp:\n",
    "    cloud_percentages = pickle.load(fp)\n",
    "\n",
    "pretty_names_methods = {\n",
    "    \"VPint\":\"VPint2\",\n",
    "    \"VPint_no_IP\":\"VPint2 (no IP)\",\n",
    "    \"VPint_no_EB\":\"VPint2 (no EB)\",\n",
    "    \"regression\":\"AutoML regression\",\n",
    "    \"regression_band\":\"AutoML regression\",\n",
    "    \"replacement\":\"Replacement\"\n",
    "}\n",
    "\n",
    "plot_kwargs_methods = {\n",
    "    \"VPint\":{'color':'#377eb8'},#, 'marker':'o'},\n",
    "    \"VPint_no_IP\":{'color':'#e41a1c'},#, 'marker':'v'},\n",
    "    \"VPint_no_EB\":{'color':'#4daf4a'},#, 'marker':'^'},\n",
    "    \"regression\":{'color':'#984ea3'},#, 'marker':'s'},\n",
    "    \"regression_band\":{'color':'#984ea3'},#, 'marker':'s'},\n",
    "    \"replacement\":{'color':'#ff7f00'},#, 'marker':'P'},\n",
    "}\n",
    "\n",
    "features = [\"1w\", \"1m\", \"3m\", \"6m\"] \n",
    "#features = [\" 1m\"]\n",
    "algs = [\"VPint\", \"VPint_no_IP\", \"VPint_no_EB\", \"replacement\", \"regression\"]\n",
    "algs = [\"VPint\", \"regression\", \"replacement\"]\n",
    "measure = \"MAE\"\n",
    "sample = False\n",
    "sample_proportion = 0.005\n",
    "\n",
    "yrange_max = 1200\n",
    "\n",
    "\n",
    "for alg in algs:\n",
    "    total_vals = np.array([])\n",
    "    total_props = []\n",
    "    for scene in scenes:\n",
    "        path = result_path + scene + \"/\"\n",
    "        for feature in features:\n",
    "            valset = []\n",
    "            path1 = path + alg + \"_\" + measure + \".csv\"\n",
    "            df = pd.read_csv(path1)\n",
    "            df = df.drop_duplicates(subset=[\"patch\"])\n",
    "            # Drop patches where coverage was not complete (e.g., due to swath)\n",
    "            invalid_patches_local = []\n",
    "            invalid_patches_local.extend(invalid_patches[scene]['target']['coverage'])\n",
    "            invalid_patches_local.extend(invalid_patches[scene][feature]['coverage'])\n",
    "            df = df[~df['patch'].isin(invalid_patches_local)]\n",
    "            # To get rid of i/o problems where writing was off (wrote pm as patch ID). Always affects duplicate rows\n",
    "            # so will not affect performance to remove these.\n",
    "            df = df[~df.patch.astype(str).str.isnumeric()] \n",
    "            df = df[df.patch.astype(str).str[0] == 'r'] \n",
    "            vals = df[feature].values\n",
    "            patches = df['patch'].values\n",
    "            if(CI_filter):\n",
    "                # Only keep results in specified confidence interval, since input data is not flawless\n",
    "                min_val = np.nanpercentile(vals, (100-CI_interval) / 2)\n",
    "                max_val = np.nanpercentile(vals, 100 - (100-CI_interval) / 2)\n",
    "                patches = patches[vals<max_val]\n",
    "                vals = vals[vals<max_val]\n",
    "                patches = patches[vals>min_val]\n",
    "                vals = vals[vals>min_val]\n",
    "                #vals = vals[vals < 100000] for testing\n",
    "\n",
    "            if(len(vals) > 0):\n",
    "                total_vals = np.concatenate((total_vals, vals))\n",
    "                total_props.extend([int(cloud_percentages[scene][p]) for p in patches])\n",
    "\n",
    "    if(sample):\n",
    "        total_props, total_vals = zip(*random.sample(list(zip(total_props, total_vals)), int(sample_proportion * len(total_props))))\n",
    "\n",
    "    \n",
    "    xs = np.arange(start=0, stop=101) # stop is exclusive it seems\n",
    "    #ys = np.zeros(101) # exclusive so 101\n",
    "    ys = np.ones((101, len(total_vals))) * np.nan # Use matrix to compute median, std etc\n",
    "    y_indices = np.zeros(len(total_vals)).astype(int) # index per x where to put new vals on axis=1 (because can't add+div)\n",
    "    x_counts = np.zeros(101) # exclusive so 101\n",
    "    for i in range(0, len(total_vals)):\n",
    "        prop = int(total_props[i])\n",
    "        val = total_vals[i]\n",
    "        ys[prop, y_indices[prop]] = val\n",
    "        x_counts[prop] += 1\n",
    "        y_indices[prop] += 1\n",
    "    xs = xs[x_counts>0]\n",
    "    xs = xs[:-1]\n",
    "    ys = ys[:-1]\n",
    "    x_counts = x_counts[:-1]\n",
    "\n",
    "    ys_median = np.nanmean(ys, axis=1) # using mean instead of median, don't want to change var names\n",
    "    ys_std = np.nanstd(ys, axis=1) / 10\n",
    "    ys_median = ys_median[x_counts>0]\n",
    "    ys_std = ys_std[x_counts>0]\n",
    "    #x_counts = x_counts[x_counts>0]\n",
    "    #ys = ys / x_counts\n",
    "\n",
    "    xnew = np.linspace(xs.min(), xs.max(), 20) \n",
    "    spl = make_interp_spline(xs, ys_median, k=3) \n",
    "    ys_median_smooth = spl(xnew)\n",
    "    spl = make_interp_spline(xs, ys_std, k=3) \n",
    "    ys_std_smooth = spl(xnew)\n",
    "    ys_std_smooth[ys_std_smooth<0] = 0\n",
    "    ys_std_smooth[ys_std_smooth > yrange_max] = yrange_max\n",
    "\n",
    "    plt.errorbar(xnew, ys_median_smooth, ys_std_smooth, label=pretty_names_methods[alg], \n",
    "                 capsize=5.0, errorevery=1, **plot_kwargs_methods[alg])\n",
    "    #plt.plot(xnew, ys_smooth, label=pretty_names_methods[alg], **plot_kwargs_methods[alg])\n",
    "    #plt.plot(xs, ys, label=pretty_names_methods[alg], **plot_kwargs_methods[alg])\n",
    "        \n",
    "    #plt.scatter(total_props, total_vals, label=pretty_names_methods[alg], **plot_kwargs_methods[alg])\n",
    "\n",
    "plt.xlabel(\"Cloud cover percentage\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.title(\"Mean MAE per cloud cover percentage, with STD\")\n",
    "#plt.xscale(\"log\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.xlim((0, 100))\n",
    "plt.ylim((0, yrange_max))\n",
    "plt.legend()\n",
    "plt.savefig(\"plots/var_cloud_percentage.pdf\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f46e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computational efficiency plot\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base_path = \"/mnt/c/Users/laure/Data/results/results_efficiency/\"\n",
    "save_path = \"results/computational_efficiency.pdf\"\n",
    "\n",
    "methods = {\n",
    "    \"VPint\": \"VPint2\",\n",
    "    \"VPint_multi\": \"VPint2 parallellised\",\n",
    "    \"regression_band\": \"AutoML regression\",\n",
    "    \"replacement\": \"Temporal replacement\",\n",
    "    \"NSPI\": \"NSPI\",\n",
    "    \"WLR\": \"WLR\",\n",
    "}\n",
    "\n",
    "num_bins = 10\n",
    "step_size = 100 / num_bins\n",
    "\n",
    "\n",
    "for m, nice_name in methods.items():\n",
    "    res_path = base_path + m + \".csv\"\n",
    "    df = pd.read_csv(res_path)\n",
    "\n",
    "    means = []\n",
    "    stds = []\n",
    "    bins = []\n",
    "\n",
    "    for b in range(0, num_bins):\n",
    "        bin_min = step_size * b\n",
    "        bin_max = step_size * (b+1)\n",
    "\n",
    "        vals = df[df[\"clp\"]>=bin_min]\n",
    "        vals = df[df[\"clp\"]<bin_max]\n",
    "        vals = vals[\"running_time\"].values\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals)\n",
    "        means.append(mean)\n",
    "        stds.append(std)\n",
    "        bins.append((bin_max + bin_min) / 2)\n",
    "\n",
    "    means = np.array(means)\n",
    "    stds = np.array(stds)\n",
    "    plt.plot(bins, means, label=nice_name)\n",
    "    #plt.errorbar(bins, means, stds, label=nice_name)\n",
    "    #plt.fill_between(bins, (means-stds), (means+stds), alpha=0.1)\n",
    "\n",
    "plt.xlabel(\"Cloud cover percentage\")\n",
    "plt.ylabel(\"Running time in seconds (log scale)\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Average running time per cloud cover percentage\")\n",
    "plt.legend()\n",
    "plt.savefig(save_path, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af91025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEN12MS-CR-TS results\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "metrics = [\"RMSE\", \"SAM\", \"PSNR\", \"SSIM\"]\n",
    "methods = [\"VPint\", \"VPint_multi\"]\n",
    "methods = [\"VPint_multi\"]\n",
    "result_path = \"/mnt/c/Users/laure/Downloads/grace/performance_measures_SEN12MS_CR_TS/\"\n",
    "result_path = \"/mnt/c/Users/laure/Downloads/grace/performance_measures_SEN12MS_CR_TS_SeNSeI_WIP/\"\n",
    "\n",
    "for metric in metrics:\n",
    "    print(\"\\n\")\n",
    "    print(metric)\n",
    "\n",
    "    for m in methods:\n",
    "        print(m)\n",
    "        results = np.array([], dtype=np.float32)\n",
    "        for geo in os.listdir(result_path):\n",
    "            local_path = result_path + geo + \"/\"\n",
    "            for roi in os.listdir(local_path):\n",
    "                local_path2 = local_path + roi + \"/\"\n",
    "                for roi_num in os.listdir(local_path2):\n",
    "                    local_path3 = local_path2 + roi_num + \"/\"\n",
    "                    res_file = local_path3 + m + \"_\" + metric + \".csv\"\n",
    "                    df_vals = pd.read_csv(res_file)[metric].values\n",
    "                    results = np.concatenate([results, df_vals])\n",
    "        print(np.nanmean(results))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoRTM_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
